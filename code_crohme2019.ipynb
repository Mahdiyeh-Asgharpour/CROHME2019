{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gaqBabKgYEBz",
        "outputId": "efe7139a-5ada-48ed-ea45-058c4aaf1f1c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/kaggle/input/crohme2019'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#download the dataset\n",
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"ntcuong2103/crohme2019\")\n",
        "path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# authored by me for parsing and rendering InkML handwriting data\n",
        "\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image, display\n",
        "from glob import glob\n",
        "\n",
        "# parse an InkML file and optionally save the rendered trace image\n",
        "def parse_and_render_inkml(file_path, save_path=None):\n",
        "    \"\"\"\n",
        "    Parses an InkML (.inkml) file, extracts stroke data (traces),\n",
        "    renders it as a handwriting image using matplotlib,\n",
        "    and optionally saves the image to a file.\n",
        "    \"\"\"\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    traces = []\n",
        "    # extract all <trace> elements\n",
        "    for trace in root.findall(\".//{http://www.w3.org/2003/InkML}trace\"):\n",
        "        raw_points = trace.text.strip().split(',')\n",
        "        stroke = []\n",
        "        for point in raw_points:\n",
        "            coords = point.strip().split()\n",
        "            if len(coords) == 2:\n",
        "                stroke.append([float(coords[0]), float(coords[1])])\n",
        "        if stroke:\n",
        "            traces.append(np.array(stroke))\n",
        "\n",
        "    # create a figure and draw the strokes\n",
        "    fig, ax = plt.subplots()\n",
        "    for stroke in traces:\n",
        "        ax.plot(stroke[:, 0], -stroke[:, 1], linewidth=2)  # flip y-axis\n",
        "\n",
        "    ax.axis('off')\n",
        "    ax.set_aspect('equal')\n",
        "\n",
        "    # save the figure if path provided, otherwise show it\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
        "        plt.close(fig)\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# explore folder contents and count files with optional extension\n",
        "def explore_directory(dir_path, extension=None):\n",
        "    print(f\"Exploring directory: {dir_path}\\n\")\n",
        "    if not os.path.exists(dir_path):\n",
        "        print(\"Path does not exist.\")\n",
        "        return\n",
        "\n",
        "    subdirs = [d for d in os.listdir(dir_path) if os.path.isdir(os.path.join(dir_path, d))]\n",
        "    files = [f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]\n",
        "\n",
        "    print(f\"Subfolders: {len(subdirs)}\")\n",
        "    for d in subdirs:\n",
        "        print(\"  └──\", d)\n",
        "\n",
        "    if extension:\n",
        "        matching = [f for f in files if f.endswith(extension)]\n",
        "        print(f\"\\n .{extension} files: {len(matching)}\")\n",
        "    else:\n",
        "        print(f\"\\n Total files: {len(files)}\")\n",
        "\n",
        "# check contents of all relevant data folders\n",
        "explore_directory(\"/kaggle/input/crohme2019/crohme2019/crohme2019/test/\", \".inkml\")\n",
        "explore_directory(\"/kaggle/input/crohme2019/crohme2019/crohme2019/train/\", \".inkml\")\n",
        "explore_directory(\"/kaggle/input/crohme2019/crohme2019/crohme2019/valid/\", \".inkml\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYX3JffW1DOP",
        "outputId": "4d497809-1798-42f9-c86a-a07af7ef7f94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exploring directory: /kaggle/input/crohme2019/crohme2019/crohme2019/test/\n",
            "\n",
            "Subfolders: 0\n",
            "\n",
            " ..inkml files: 1199\n",
            "Exploring directory: /kaggle/input/crohme2019/crohme2019/crohme2019/train/\n",
            "\n",
            "Subfolders: 0\n",
            "\n",
            " ..inkml files: 8901\n",
            "Exploring directory: /kaggle/input/crohme2019/crohme2019/crohme2019/valid/\n",
            "\n",
            "Subfolders: 0\n",
            "\n",
            " ..inkml files: 986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parsing label file and saving structured labels to CSV\n",
        "import pandas as pd\n",
        "\n",
        "# this is the path to the ground-truth label file\n",
        "label_file_path = \"/kaggle/input/crohme2019/crohme2019_train.txt\"\n",
        "\n",
        "# list to hold parsed (file_path, label_text) pairs\n",
        "label_entries = []\n",
        "\n",
        "# read each line from the label file and split into components\n",
        "with open(label_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line:  # skip empty lines\n",
        "            parts = line.split(maxsplit=1)  # split into path and label\n",
        "            if len(parts) == 2:\n",
        "                label_entries.append(parts)\n",
        "\n",
        "# convert parsed data into a DataFrame\n",
        "df = pd.DataFrame(label_entries, columns=[\"file_path\", \"label\"])\n",
        "\n",
        "# extract just the file name from the path (for easier matching later)\n",
        "df[\"file_name\"] = df[\"file_path\"].apply(lambda x: x.split(\"/\")[-1])\n",
        "\n",
        "# tokenize label string into list of tokens\n",
        "df[\"tokens\"] = df[\"label\"].str.split()\n",
        "\n",
        "# export the DataFrame to a CSV file for later use\n",
        "df.to_csv(\"/kaggle/working/train_labels.csv\", index=False)\n",
        "print(\" Label data saved to train_labels.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ymkP804w5Gk",
        "outputId": "fd83f3c9-a78d-4c06-82c4-136cd41ede69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label data saved to train_labels.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parsing label file and saving structured labels to CSV\n",
        "import pandas as pd\n",
        "\n",
        "# this is the path to the ground-truth label file for test set\n",
        "label_file_path = \"/content/crohme2019_test.txt\"\n",
        "\n",
        "# list to hold parsed (file_path, label_text) pairs\n",
        "label_entries = []\n",
        "\n",
        "# read each line from the label file and split into components\n",
        "with open(label_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line:  # skip empty lines\n",
        "            parts = line.split(maxsplit=1)  # split into path and label\n",
        "            if len(parts) == 2:\n",
        "                label_entries.append(parts)\n",
        "\n",
        "# convert parsed data into a DataFrame\n",
        "df = pd.DataFrame(label_entries, columns=[\"file_path\", \"label\"])\n",
        "\n",
        "# extract just the file name from the path (for easier matching later)\n",
        "df[\"file_name\"] = df[\"file_path\"].apply(lambda x: x.split(\"/\")[-1])\n",
        "\n",
        "# tokenize label string into list of tokens\n",
        "df[\"tokens\"] = df[\"label\"].str.split()\n",
        "\n",
        "# export the DataFrame to a CSV file for later use\n",
        "df.to_csv(\"test_labels.csv\", index=False)\n",
        "print(\" Label data saved to test_labels.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTLWCa3kw5oI",
        "outputId": "a9827303-9208-4fd6-e0f7-c5c8fcda0163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label data saved to test_labels.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parsing label file and saving structured labels to CSV\n",
        "import pandas as pd\n",
        "\n",
        "# this is the path to the ground-truth label file for validation set\n",
        "label_file_path = \"/content/crohme2019_valid.txt\"\n",
        "\n",
        "# list to hold parsed (file_path, label_text) pairs\n",
        "label_entries = []\n",
        "\n",
        "# read each line from the label file and split into components\n",
        "with open(label_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line:  # skip empty lines\n",
        "            parts = line.split(maxsplit=1)  # split into path and label\n",
        "            if len(parts) == 2:\n",
        "                label_entries.append(parts)\n",
        "\n",
        "# convert parsed data into a DataFrame\n",
        "df = pd.DataFrame(label_entries, columns=[\"file_path\", \"label\"])\n",
        "\n",
        "# extract just the file name from the path (for easier matching later)\n",
        "df[\"file_name\"] = df[\"file_path\"].apply(lambda x: x.split(\"/\")[-1])\n",
        "\n",
        "# tokenize label string into list of tokens\n",
        "df[\"tokens\"] = df[\"label\"].str.split()\n",
        "\n",
        "# export the DataFrame to a CSV file for later use\n",
        "df.to_csv(\"valid_labels.csv\", index=False)\n",
        "print(\" Label data saved to valid_labels.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7AwLUVaxy_B",
        "outputId": "fe3d0cd7-6a1c-42dd-a21d-83733577b3ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label data saved to valid_labels.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import json\n",
        "\n",
        "# load the CSV file containing training labels and their tokenized math expressions\n",
        "train_df = pd.read_csv(\"/content/train_labels.csv\")\n",
        "\n",
        "# in case the token column doesn't exist, generate it from the label string\n",
        "if \"tokens\" not in train_df.columns:\n",
        "    train_df[\"tokens\"] = train_df[\"label\"].astype(str).str.split()\n",
        "\n",
        "# flatten all tokens across all samples into a single list\n",
        "# this allows us to find all unique tokens used in the training data\n",
        "all_tokens = [token for tokens in train_df[\"tokens\"] for token in tokens if isinstance(token, str)]\n",
        "\n",
        "# count frequency of each token (can be useful for analysis)\n",
        "token_counts = Counter(all_tokens)\n",
        "\n",
        "# define special tokens used by the model (padding, start/end, unknown)\n",
        "special_tokens = [\"<PAD>\", \"<SOS>\", \"<EOS>\", \"<UNK>\"]\n",
        "\n",
        "# collect all unique tokens from the dataset and sort them alphabetically\n",
        "unique_tokens = sorted(set(all_tokens))\n",
        "\n",
        "# construct the final vocabulary list: special tokens first, then all unique tokens\n",
        "vocab = special_tokens + unique_tokens\n",
        "\n",
        "# create a dictionary that maps each token to a unique integer ID\n",
        "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
        "\n",
        "# create the reverse mapping: ID back to token (used during decoding)\n",
        "id_to_token = {idx: token for token, idx in token_to_id.items()}\n",
        "\n",
        "# export the token-to-ID mapping to a JSON file for later use during training\n",
        "with open(\"token_to_id.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(token_to_id, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# also export the reverse mapping (optional but useful for evaluation/inference)\n",
        "with open(\"id_to_token.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(id_to_token, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# confirmation message\n",
        "print(f\" Vocabulary saved! Total tokens: {len(vocab)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeEuId_s24ru",
        "outputId": "3e7c5ea5-15d0-4acb-8430-ea51430d3434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Vocabulary saved! Total tokens: 77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "\n",
        "# load token-to-id mapping from previously generated vocab\n",
        "with open(\"token_to_id.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    token_to_id = json.load(f)\n",
        "\n",
        "# helper function to encode tokens using vocab (with <UNK> fallback)\n",
        "def encode_tokens(tokens, vocab):\n",
        "    return [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens]\n",
        "\n",
        "# process label CSV and add encoded sequences\n",
        "def encode_label_file(csv_path):\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"File not found: {csv_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Processing: {csv_path}\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # tokenize if necessary\n",
        "    if \"tokens\" not in df.columns:\n",
        "        df[\"tokens\"] = df[\"label\"].astype(str).str.split()\n",
        "\n",
        "    # encode tokens to integer IDs\n",
        "    df[\"label_ids\"] = df[\"tokens\"].apply(lambda tokens: encode_tokens(tokens, token_to_id))\n",
        "    df[\"label_ids_with_sos_eos\"] = df[\"label_ids\"].apply(\n",
        "        lambda ids: [token_to_id[\"<SOS>\"]] + ids + [token_to_id[\"<EOS>\"]]\n",
        "    )\n",
        "\n",
        "    # save back to same CSV\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"Saved encoded labels to: {csv_path}\\n\")\n",
        "\n",
        "# paths to all 3 label files\n",
        "label_files = [\n",
        "    \"train_labels.csv\",\n",
        "    \"valid_labels.csv\",\n",
        "    \"test_labels.csv\"\n",
        "]\n",
        "\n",
        "# apply encoding to all label files\n",
        "for file_path in label_files:\n",
        "    encode_label_file(file_path)\n"
      ],
      "metadata": {
        "id": "yAjFFExjU9cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28bb0211-1335-41b3-b7fb-11ac0a0500d0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: train_labels.csv\n",
            "Saved encoded labels to: train_labels.csv\n",
            "\n",
            "Processing: valid_labels.csv\n",
            "Saved encoded labels to: valid_labels.csv\n",
            "\n",
            "Processing: test_labels.csv\n",
            "Saved encoded labels to: test_labels.csv\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}