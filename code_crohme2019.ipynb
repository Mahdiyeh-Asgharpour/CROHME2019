{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gaqBabKgYEBz",
        "outputId": "efe7139a-5ada-48ed-ea45-058c4aaf1f1c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/kaggle/input/crohme2019'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "#download the dataset\n",
        "import kagglehub\n",
        "path = kagglehub.dataset_download(\"ntcuong2103/crohme2019\")\n",
        "path"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# authored by me for parsing and rendering InkML handwriting data\n",
        "\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Image, display\n",
        "from glob import glob\n",
        "\n",
        "# parse an InkML file and optionally save the rendered trace image\n",
        "def parse_and_render_inkml(file_path, save_path=None):\n",
        "    \"\"\"\n",
        "    Parses an InkML (.inkml) file, extracts stroke data (traces),\n",
        "    renders it as a handwriting image using matplotlib,\n",
        "    and optionally saves the image to a file.\n",
        "    \"\"\"\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    traces = []\n",
        "    # extract all <trace> elements\n",
        "    for trace in root.findall(\".//{http://www.w3.org/2003/InkML}trace\"):\n",
        "        raw_points = trace.text.strip().split(',')\n",
        "        stroke = []\n",
        "        for point in raw_points:\n",
        "            coords = point.strip().split()\n",
        "            if len(coords) == 2:\n",
        "                stroke.append([float(coords[0]), float(coords[1])])\n",
        "        if stroke:\n",
        "            traces.append(np.array(stroke))\n",
        "\n",
        "    # create a figure and draw the strokes\n",
        "    fig, ax = plt.subplots()\n",
        "    for stroke in traces:\n",
        "        ax.plot(stroke[:, 0], -stroke[:, 1], linewidth=2)  # flip y-axis\n",
        "\n",
        "    ax.axis('off')\n",
        "    ax.set_aspect('equal')\n",
        "\n",
        "    # save the figure if path provided, otherwise show it\n",
        "    if save_path:\n",
        "        plt.savefig(save_path, bbox_inches='tight', pad_inches=0)\n",
        "        plt.close(fig)\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "# explore folder contents and count files with optional extension\n",
        "def explore_directory(dir_path, extension=None):\n",
        "    print(f\"Exploring directory: {dir_path}\\n\")\n",
        "    if not os.path.exists(dir_path):\n",
        "        print(\"Path does not exist.\")\n",
        "        return\n",
        "\n",
        "    subdirs = [d for d in os.listdir(dir_path) if os.path.isdir(os.path.join(dir_path, d))]\n",
        "    files = [f for f in os.listdir(dir_path) if os.path.isfile(os.path.join(dir_path, f))]\n",
        "\n",
        "    print(f\"Subfolders: {len(subdirs)}\")\n",
        "    for d in subdirs:\n",
        "        print(\"  └──\", d)\n",
        "\n",
        "    if extension:\n",
        "        matching = [f for f in files if f.endswith(extension)]\n",
        "        print(f\"\\n .{extension} files: {len(matching)}\")\n",
        "    else:\n",
        "        print(f\"\\n Total files: {len(files)}\")\n",
        "\n",
        "# check contents of all relevant data folders\n",
        "explore_directory(\"/kaggle/input/crohme2019/crohme2019/crohme2019/test/\", \".inkml\")\n",
        "explore_directory(\"/kaggle/input/crohme2019/crohme2019/crohme2019/train/\", \".inkml\")\n",
        "explore_directory(\"/kaggle/input/crohme2019/crohme2019/crohme2019/valid/\", \".inkml\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YYX3JffW1DOP",
        "outputId": "4d497809-1798-42f9-c86a-a07af7ef7f94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exploring directory: /kaggle/input/crohme2019/crohme2019/crohme2019/test/\n",
            "\n",
            "Subfolders: 0\n",
            "\n",
            " ..inkml files: 1199\n",
            "Exploring directory: /kaggle/input/crohme2019/crohme2019/crohme2019/train/\n",
            "\n",
            "Subfolders: 0\n",
            "\n",
            " ..inkml files: 8901\n",
            "Exploring directory: /kaggle/input/crohme2019/crohme2019/crohme2019/valid/\n",
            "\n",
            "Subfolders: 0\n",
            "\n",
            " ..inkml files: 986\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parsing label file and saving structured labels to CSV\n",
        "import pandas as pd\n",
        "\n",
        "# this is the path to the ground-truth label file\n",
        "label_file_path = \"/kaggle/input/crohme2019/crohme2019_train.txt\"\n",
        "\n",
        "# list to hold parsed (file_path, label_text) pairs\n",
        "label_entries = []\n",
        "\n",
        "# read each line from the label file and split into components\n",
        "with open(label_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line:  # skip empty lines\n",
        "            parts = line.split(maxsplit=1)  # split into path and label\n",
        "            if len(parts) == 2:\n",
        "                label_entries.append(parts)\n",
        "\n",
        "# convert parsed data into a DataFrame\n",
        "df = pd.DataFrame(label_entries, columns=[\"file_path\", \"label\"])\n",
        "\n",
        "# extract just the file name from the path (for easier matching later)\n",
        "df[\"file_name\"] = df[\"file_path\"].apply(lambda x: x.split(\"/\")[-1])\n",
        "\n",
        "# tokenize label string into list of tokens\n",
        "df[\"tokens\"] = df[\"label\"].str.split()\n",
        "\n",
        "# export the DataFrame to a CSV file for later use\n",
        "df.to_csv(\"/kaggle/working/train_labels.csv\", index=False)\n",
        "print(\" Label data saved to train_labels.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ymkP804w5Gk",
        "outputId": "fd83f3c9-a78d-4c06-82c4-136cd41ede69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label data saved to train_labels.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parsing label file and saving structured labels to CSV\n",
        "import pandas as pd\n",
        "\n",
        "# this is the path to the ground-truth label file for test set\n",
        "label_file_path = \"/content/crohme2019_test.txt\"\n",
        "\n",
        "# list to hold parsed (file_path, label_text) pairs\n",
        "label_entries = []\n",
        "\n",
        "# read each line from the label file and split into components\n",
        "with open(label_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line:  # skip empty lines\n",
        "            parts = line.split(maxsplit=1)  # split into path and label\n",
        "            if len(parts) == 2:\n",
        "                label_entries.append(parts)\n",
        "\n",
        "# convert parsed data into a DataFrame\n",
        "df = pd.DataFrame(label_entries, columns=[\"file_path\", \"label\"])\n",
        "\n",
        "# extract just the file name from the path (for easier matching later)\n",
        "df[\"file_name\"] = df[\"file_path\"].apply(lambda x: x.split(\"/\")[-1])\n",
        "\n",
        "# tokenize label string into list of tokens\n",
        "df[\"tokens\"] = df[\"label\"].str.split()\n",
        "\n",
        "# export the DataFrame to a CSV file for later use\n",
        "df.to_csv(\"test_labels.csv\", index=False)\n",
        "print(\" Label data saved to test_labels.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTLWCa3kw5oI",
        "outputId": "a9827303-9208-4fd6-e0f7-c5c8fcda0163"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label data saved to test_labels.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parsing label file and saving structured labels to CSV\n",
        "import pandas as pd\n",
        "\n",
        "# this is the path to the ground-truth label file for validation set\n",
        "label_file_path = \"/content/crohme2019_valid.txt\"\n",
        "\n",
        "# list to hold parsed (file_path, label_text) pairs\n",
        "label_entries = []\n",
        "\n",
        "# read each line from the label file and split into components\n",
        "with open(label_file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if line:  # skip empty lines\n",
        "            parts = line.split(maxsplit=1)  # split into path and label\n",
        "            if len(parts) == 2:\n",
        "                label_entries.append(parts)\n",
        "\n",
        "# convert parsed data into a DataFrame\n",
        "df = pd.DataFrame(label_entries, columns=[\"file_path\", \"label\"])\n",
        "\n",
        "# extract just the file name from the path (for easier matching later)\n",
        "df[\"file_name\"] = df[\"file_path\"].apply(lambda x: x.split(\"/\")[-1])\n",
        "\n",
        "# tokenize label string into list of tokens\n",
        "df[\"tokens\"] = df[\"label\"].str.split()\n",
        "\n",
        "# export the DataFrame to a CSV file for later use\n",
        "df.to_csv(\"valid_labels.csv\", index=False)\n",
        "print(\" Label data saved to valid_labels.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7AwLUVaxy_B",
        "outputId": "fe3d0cd7-6a1c-42dd-a21d-83733577b3ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label data saved to valid_labels.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import json\n",
        "\n",
        "# load the CSV file containing training labels and their tokenized math expressions\n",
        "train_df = pd.read_csv(\"/content/train_labels.csv\")\n",
        "\n",
        "# in case the token column doesn't exist, generate it from the label string\n",
        "if \"tokens\" not in train_df.columns:\n",
        "    train_df[\"tokens\"] = train_df[\"label\"].astype(str).str.split()\n",
        "\n",
        "# flatten all tokens across all samples into a single list\n",
        "# this allows us to find all unique tokens used in the training data\n",
        "all_tokens = [token for tokens in train_df[\"tokens\"] for token in tokens if isinstance(token, str)]\n",
        "\n",
        "# count frequency of each token (can be useful for analysis)\n",
        "token_counts = Counter(all_tokens)\n",
        "\n",
        "# define special tokens used by the model (padding, start/end, unknown)\n",
        "special_tokens = [\"<PAD>\", \"<SOS>\", \"<EOS>\", \"<UNK>\"]\n",
        "\n",
        "# collect all unique tokens from the dataset and sort them alphabetically\n",
        "unique_tokens = sorted(set(all_tokens))\n",
        "\n",
        "# construct the final vocabulary list: special tokens first, then all unique tokens\n",
        "vocab = special_tokens + unique_tokens\n",
        "\n",
        "# create a dictionary that maps each token to a unique integer ID\n",
        "token_to_id = {token: idx for idx, token in enumerate(vocab)}\n",
        "\n",
        "# create the reverse mapping: ID back to token (used during decoding)\n",
        "id_to_token = {idx: token for token, idx in token_to_id.items()}\n",
        "\n",
        "# export the token-to-ID mapping to a JSON file for later use during training\n",
        "with open(\"token_to_id.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(token_to_id, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# also export the reverse mapping (optional but useful for evaluation/inference)\n",
        "with open(\"id_to_token.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(id_to_token, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "# confirmation message\n",
        "print(f\" Vocabulary saved! Total tokens: {len(vocab)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeEuId_s24ru",
        "outputId": "3e7c5ea5-15d0-4acb-8430-ea51430d3434"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Vocabulary saved! Total tokens: 77\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import os\n",
        "\n",
        "# load token-to-id mapping from previously generated vocab\n",
        "with open(\"token_to_id.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "    token_to_id = json.load(f)\n",
        "\n",
        "# helper function to encode tokens using vocab (with <UNK> fallback)\n",
        "def encode_tokens(tokens, vocab):\n",
        "    return [vocab.get(token, vocab[\"<UNK>\"]) for token in tokens]\n",
        "\n",
        "# process label CSV and add encoded sequences\n",
        "def encode_label_file(csv_path):\n",
        "    if not os.path.exists(csv_path):\n",
        "        print(f\"File not found: {csv_path}\")\n",
        "        return\n",
        "\n",
        "    print(f\"Processing: {csv_path}\")\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # tokenize if necessary\n",
        "    if \"tokens\" not in df.columns:\n",
        "        df[\"tokens\"] = df[\"label\"].astype(str).str.split()\n",
        "\n",
        "    # encode tokens to integer IDs\n",
        "    df[\"label_ids\"] = df[\"tokens\"].apply(lambda tokens: encode_tokens(tokens, token_to_id))\n",
        "    df[\"label_ids_with_sos_eos\"] = df[\"label_ids\"].apply(\n",
        "        lambda ids: [token_to_id[\"<SOS>\"]] + ids + [token_to_id[\"<EOS>\"]]\n",
        "    )\n",
        "\n",
        "    # save back to same CSV\n",
        "    df.to_csv(csv_path, index=False)\n",
        "    print(f\"Saved encoded labels to: {csv_path}\\n\")\n",
        "\n",
        "# paths to all 3 label files\n",
        "label_files = [\n",
        "    \"train_labels.csv\",\n",
        "    \"valid_labels.csv\",\n",
        "    \"test_labels.csv\"\n",
        "]\n",
        "\n",
        "# apply encoding to all label files\n",
        "for file_path in label_files:\n",
        "    encode_label_file(file_path)\n"
      ],
      "metadata": {
        "id": "yAjFFExjU9cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28bb0211-1335-41b3-b7fb-11ac0a0500d0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing: train_labels.csv\n",
            "Saved encoded labels to: train_labels.csv\n",
            "\n",
            "Processing: valid_labels.csv\n",
            "Saved encoded labels to: valid_labels.csv\n",
            "\n",
            "Processing: test_labels.csv\n",
            "Saved encoded labels to: test_labels.csv\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from xml.etree import ElementTree as ET\n",
        "from glob import glob\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "\n",
        "# Paths\n",
        "INKML_FOLDER = \"/kaggle/input/crohme2019/crohme2019/crohme2019/train/\"\n",
        "OUTPUT_FOLDER = \"train images/\"\n",
        "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
        "IMG_SIZE = (128, 128)\n",
        "\n",
        "# Parse InkML file and extract list of valid strokes\n",
        "def parse_inkml_traces(file_path):\n",
        "    traces = []\n",
        "    try:\n",
        "        tree = ET.parse(file_path)\n",
        "        root = tree.getroot()\n",
        "        for trace in root.findall(\".//{http://www.w3.org/2003/InkML}trace\"):\n",
        "            raw = trace.text.strip().split(',')\n",
        "            stroke = []\n",
        "            for pt in raw:\n",
        "                coords = pt.strip().split()\n",
        "                if len(coords) == 2:\n",
        "                    stroke.append([float(coords[0]), float(coords[1])])\n",
        "            if len(stroke) >= 2:\n",
        "                traces.append(np.array(stroke))\n",
        "    except Exception as e:\n",
        "        raise ValueError(f\"Could not parse {file_path}: {e}\")\n",
        "    return traces\n",
        "\n",
        "# Render strokes onto canvas and preprocess the image\n",
        "def render_image(traces, augment=True):\n",
        "    canvas = np.ones((600, 600), dtype=np.uint8) * 255  # white background\n",
        "\n",
        "    for stroke in traces:\n",
        "        for i in range(len(stroke) - 1):\n",
        "            pt1 = tuple(map(int, stroke[i]))\n",
        "            pt2 = tuple(map(int, stroke[i + 1]))\n",
        "            cv2.line(canvas, pt1, pt2, color=0, thickness=2)\n",
        "\n",
        "    # resize whole canvas directly (no cropping)\n",
        "    resized = cv2.resize(canvas, IMG_SIZE, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "    if augment:\n",
        "        angle = random.uniform(-5, 5)\n",
        "        M = cv2.getRotationMatrix2D((IMG_SIZE[1] // 2, IMG_SIZE[0] // 2), angle, 1)\n",
        "        resized = cv2.warpAffine(resized, M, IMG_SIZE, borderValue=255)\n",
        "\n",
        "    return resized.astype(np.float32) / 255.0  # normalize to [0, 1]\n",
        "\n",
        "# Process all .inkml files and render them to PNG\n",
        "inkml_files = glob(os.path.join(INKML_FOLDER, \"*.inkml\"))\n",
        "print(f\"Found {len(inkml_files)} .inkml files to process.\")\n",
        "\n",
        "failed_files = []\n",
        "\n",
        "for inkml_path in tqdm(inkml_files):\n",
        "    file_name = os.path.splitext(os.path.basename(inkml_path))[0]\n",
        "    output_path = os.path.join(OUTPUT_FOLDER, file_name + \".png\")\n",
        "\n",
        "    try:\n",
        "        traces = parse_inkml_traces(inkml_path)\n",
        "        image = render_image(traces, augment=True)\n",
        "        image_uint8 = (image * 255).astype(np.uint8)\n",
        "        cv2.imwrite(output_path, image_uint8)\n",
        "\n",
        "    except Exception as e:\n",
        "        failed_files.append(file_name)\n",
        "        print(f\"Failed to render {file_name}: {e}\")\n",
        "\n",
        "print(f\"\\n Total files that failed to render: {len(failed_files)}\")\n",
        "if failed_files:\n",
        "    print(\"Example failed files:\", failed_files[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_iTfWu3KxhY5",
        "outputId": "a12759ff-fcf4-4a75-b249-dc0e569bc6d7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 8901 .inkml files to process.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 48%|████▊     | 4284/8901 [00:19<00:20, 230.19it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Failed to render MfrDB0104: Could not parse /kaggle/input/crohme2019/crohme2019/crohme2019/train/MfrDB0104.inkml: not well-formed (invalid token): line 15, column 23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 8901/8901 [00:42<00:00, 207.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Total files that failed to render: 1\n",
            "Example failed files: ['MfrDB0104']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import json\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# path to the CSV file that contains label info\n",
        "CSV_PATH = \"/content/train_labels.csv\"\n",
        "\n",
        "# path to folder where images (PNG) are stored\n",
        "IMAGE_FOLDER = \"/content/train_images/\"\n",
        "\n",
        "# path to vocab file for reading the PAD token ID\n",
        "VOCAB_FILE = \"/content/token_to_id.json\"\n",
        "\n",
        "# final input size expected by the model\n",
        "IMG_SIZE = (128, 128)\n",
        "\n",
        "# load the label CSV file\n",
        "df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "# load the vocab to get the PAD token index\n",
        "with open(VOCAB_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "    token_to_id = json.load(f)\n",
        "pad_token_id = token_to_id[\"<PAD>\"]\n",
        "\n",
        "# make sure label column is present\n",
        "if \"label_ids_with_sos_eos\" not in df.columns:\n",
        "    raise ValueError(\"Missing label_ids_with_sos_eos column. Please run encoding first.\")\n",
        "\n",
        "# convert stringified list in CSV to actual list of integers\n",
        "def str_to_list(s):\n",
        "    return list(map(int, s.strip(\"[]\").split(\",\")))\n",
        "\n",
        "df[\"label_ids_with_sos_eos\"] = df[\"label_ids_with_sos_eos\"].apply(str_to_list)\n",
        "\n",
        "# determine max sequence length for padding\n",
        "max_seq_len = max(df[\"label_ids_with_sos_eos\"].apply(len))\n",
        "\n",
        "# build full paths to images and pad all label sequences\n",
        "image_paths = [os.path.join(IMAGE_FOLDER, fname.replace(\".inkml\", \".png\")) for fname in df[\"file_name\"]]\n",
        "label_seqs = [seq + [pad_token_id] * (max_seq_len - len(seq)) for seq in df[\"label_ids_with_sos_eos\"]]\n",
        "\n",
        "# create a raw TensorFlow dataset from file paths and labels\n",
        "dataset = tf.data.Dataset.from_tensor_slices((image_paths, label_seqs))\n",
        "\n",
        "# define a function that loads and preprocesses each image\n",
        "def process_sample(image_path, label_seq):\n",
        "    image = tf.io.read_file(image_path)                      # read file\n",
        "    image = tf.image.decode_png(image, channels=1)           # decode grayscale PNG\n",
        "    image = tf.image.convert_image_dtype(image, tf.float32)  # normalize to [0, 1]\n",
        "    image = tf.image.resize(image, IMG_SIZE)                 # ensure fixed size\n",
        "    return image, tf.cast(label_seq, tf.int32)\n",
        "\n",
        "# apply preprocessing to every item in the dataset\n",
        "dataset = dataset.map(process_sample, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# shuffle, batch, and prefetch for training\n",
        "BATCH_SIZE = 32\n",
        "train_dataset = dataset.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "lLanvOZw3omf"
      },
      "execution_count": 9,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}